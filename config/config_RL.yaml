training:
  name: "local_2_10_1e-1_4"
  total_timesteps: 10000000 # Total number of training timesteps
  log_interval: 100 # Number of timesteps between logging events
  check_freq: 1000 # Number of timesteps between saving events
  judgment_timestep: 200000 # Number of timesteps before agent is able to say if position is reachable or not

SAC:
  model_param:
    {
      policy: "MultiInputPolicy",
      verbose: 2,
      learning_starts: 1000,
      tensorboard_log: "logs/",
      device: "cpu",
      buffer_size: 1000000,
      learning_rate: 0.0003,
      train_freq: 2,
      gamma: 0.95,
      batch_size: 128,
      tau: 0.01,
      gradient_steps: 1,
      ent_coef: "auto_0.04",
      policy_kwargs: {
        net_arch: [512, 512, 512],
      },
    }
  HerReplayBuffer_param:
    replay_buffer_kwargs:
      {
        copy_info_dict: True,
        n_sampled_goal: 1,
        goal_selection_strategy: "future",
      }


environment:
  nb_environments: 1 # Number of environments used to collect data
  numSimulationSteps: 10
  timeStepSimulation: 1e-3
  normalizeObs: False
  randomInit: True # Choose to randomize the position of controlled joint arount the initial position 
  limitPosScale: 1
  limitVelScale: 1
  torqueScaleCoeff: 1
  thresholdSuccess: 0.05
  timeSuccess: 2500

  # Stop conditions
  maxTime: 300 # Maximum epoch time in seconds
  lowerLimitPos: [-0.25, -0.05, 0.9]
  upperLimitPos: [0.1, 0.05, 1.3]
  # Target (can be a fixed position or a box or a sphere)
  targetType: "sphere"
  # if fixed position only this parameter is used
  targetPosition: [0.4, 0.3, 1.05]
  # if box, around the targetPosition
  targetSizeLow: [-0.1, -0.1, -0.1]
  targetSizeHigh: [0.1, 0.1, 0.1]
  # if sphere, around the targetPosition
  targetRadius: 0.1
  
  rewardType: "mix" # "sparse", "dense" or "mix" if want to mix both
  # Currently, w_target_reached does not seems to work if not 0 (TO check), so equivalent to dense rn
  # Coeff of boolean reward : 1 if target reached, 0 otherwise times w_target_reached
  w_target_reached: 1
  # Coeff of distance reward : - distance to target times w_target_pos
  w_target_pos: 20
  # Coeff of torque reward : - torques times w_control_reg
  w_control_reg: 0.07
  # Coeff of velocity reward : 1 if alive else 0 times w_penalization_truncation
  w_penalization_truncation: 7
  # Coeff of timeon target reward : 1 if on target else 0 times w_time_on_target
  w_time_on_target: 0


robot_designer:
  URDF: "/talos_data/robots/talos_reduced.urdf"
  SRDF: "/talos_data/srdf/talos.srdf"
  controlledJoints:
    [
      # root_joint,
      # leg_left_1_joint,
      # leg_left_2_joint,
      # leg_left_3_joint,
      # leg_left_4_joint,
      # leg_left_5_joint,
      # leg_left_6_joint,
      # leg_right_1_joint,
      # leg_right_2_joint,
      # leg_right_3_joint,
      # leg_right_4_joint,
      # leg_right_5_joint,
      # leg_right_6_joint,
      torso_1_joint,
      torso_2_joint,
      arm_left_1_joint,
      arm_left_2_joint,
      arm_left_3_joint,
      arm_left_4_joint,
      arm_left_5_joint,
      arm_left_6_joint,
      arm_left_7_joint,
      # arm_right_1_joint,
      # arm_right_2_joint,
      # arm_right_3_joint,
      # arm_right_4_joint,
    ]

  # Initial position of the robot
  toolPosition: [0, -0.02, -0.0825]
